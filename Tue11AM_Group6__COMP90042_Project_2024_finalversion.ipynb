{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "689e4576-3f61-428a-81b3-bd8c53da8e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkmQ-1r-Wu53",
        "outputId": "d818bfa2-645b-4b3e-9e61-037d00c4b5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Current Working Directory:  /content/drive/My Drive/NLP Project\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/NLP Project'\n",
        "os.chdir(folder_path)\n",
        "\n",
        "print(\"Current Working Directory: \", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N3jQmZjghCuN"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rsWgUT6ahCuN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_claims = json.load(open(\"train-claims.json\", \"r\"))\n",
        "dev_claims = json.load(open(\"dev-claims.json\", \"r\"))\n",
        "test_claims = json.load(open(\"test-claims-unlabelled.json\", \"r\"))\n",
        "evidences = json.load(open(\"evidence.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-En3ELTJIM8",
        "outputId": "211fe1d7-6a80-4c82-9be9-6a9e647c63b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWbixNL5hCuN",
        "outputId": "b93f03ef-4ed9-49b0-dfce-14230b8dd6ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(word):\n",
        "    lemma = lemmatizer.lemmatize(word,'v')\n",
        "    if lemma == word:\n",
        "        lemma = lemmatizer.lemmatize(word,'n')\n",
        "    return lemma\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "stopwords = set(stopwords.words('english')) #note: stopwords are all in lowercase\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data = data.lower()\n",
        "    tokenized_words= tt.tokenize(data) #tokenized\n",
        "    remove_nonEnglish_data = [token for token in tokenized_words if re.search('[a-z]',token)]\n",
        "    remove_stopword = [token for token in remove_nonEnglish_data if token not in stopwords] #remove stopword\n",
        "    lemmatized = [lemmatize(token) for token in remove_stopword]\n",
        "\n",
        "    sentence = \" \".join(lemmatized)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABNcfAYdhCuN",
        "outputId": "c00aedf7-3d0d-47f8-df84-fbbc7b03288f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "def obtain_claim (data):\n",
        "    claims_id = []\n",
        "    claims_text = []\n",
        "    claims_label = []\n",
        "    claims_evidence = []\n",
        "    for key, value in data.items():\n",
        "        claims_id.append(key)\n",
        "        claims_text.append(preprocess_data(value[\"claim_text\"]))\n",
        "        claims_label.append(value[\"claim_label\"])\n",
        "        claims_evidence.append([evidence for evidence in value[\"evidences\"]])\n",
        "    return claims_id, claims_text, claims_label, claims_evidence\n",
        "\n",
        "evidence_text = []\n",
        "evidence_id = []\n",
        "for key, value in evidences.items():\n",
        "    processed = preprocess_data(value)\n",
        "    evidence_text.append(processed)\n",
        "    evidence_id.append(key)\n",
        "\n",
        "dev_claims_id, dev_claims_text, dev_claims_label, dev_claims_evidence = obtain_claim(dev_claims)\n",
        "train_claims_id, train_claims_text, train_claims_label, train_claims_evidence = obtain_claim(train_claims)\n",
        "\n",
        "test_claims_id = []\n",
        "test_claims_text = []\n",
        "for key, value in test_claims.items():\n",
        "    test_claims_id.append(key)\n",
        "    test_claims_text.append(preprocess_data(value[\"claim_text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N3A1Je1C21hU"
      },
      "outputs": [],
      "source": [
        "def obtain_evidence_id (data):\n",
        "    idlist = []\n",
        "    for evidences in data:\n",
        "        idlist.append([int(evidence.split('-')[1]) for evidence in evidences])\n",
        "    return idlist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l9ljHr7m22HS"
      },
      "outputs": [],
      "source": [
        "train_claims_evidence_id = obtain_evidence_id(train_claims_evidence)\n",
        "dev_claims_evidence_id = obtain_evidence_id(dev_claims_evidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vSfRcwaXhCuN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(evidence_text)\n",
        "evidence_matrix = vectorizer.transform(evidence_text)\n",
        "train_text_matrix = vectorizer.transform(train_claims_text)\n",
        "dev_text_matrix = vectorizer.transform(dev_claims_text)\n",
        "test_text_matrix = vectorizer.transform(test_claims_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DtQoWgE_hCuN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_dev = cosine_similarity(dev_text_matrix, evidence_matrix)\n",
        "similarity_test = cosine_similarity(test_text_matrix, evidence_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z9ERAhMm3cSR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def obtain_relevent_evidence_id (similarity):\n",
        "    res = []\n",
        "    for i in range(len(similarity)):\n",
        "        #select the top 1000 related evidence\n",
        "        sorted_indices = np.argsort(similarity[i])[::-1].tolist()[:100]\n",
        "        res.append(sorted_indices)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gL-KwsGVlCHo"
      },
      "outputs": [],
      "source": [
        "def compute_similarity_in_chunks(train_text_matrix, evidence_matrix, chunk_size=100):\n",
        "    n_samples_train = train_text_matrix.shape[0]\n",
        "    n_samples_evidence = evidence_matrix.shape[0]\n",
        "\n",
        "    similarity_matrix = np.zeros((n_samples_train, n_samples_evidence))\n",
        "\n",
        "    for start_idx in range(0, n_samples_train, chunk_size):\n",
        "        end_idx = min(start_idx + chunk_size, n_samples_train)\n",
        "        train_batch = train_text_matrix[start_idx:end_idx, :]\n",
        "        similarity_batch = cosine_similarity(train_batch, evidence_matrix)\n",
        "        similarity_matrix[start_idx:end_idx, :] = similarity_batch\n",
        "\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FELjo-rE4oiW"
      },
      "outputs": [],
      "source": [
        "train_claims_relevent_evidence_id = []\n",
        "for i in range(6):\n",
        "  similarity_train = compute_similarity_in_chunks(train_text_matrix[i * 200 : (i + 1) * 200], evidence_matrix)\n",
        "  train_claims_relevent_evidence_id += obtain_relevent_evidence_id (similarity_train)\n",
        "similarity_train = compute_similarity_in_chunks(train_text_matrix[1200:], evidence_matrix)\n",
        "train_claims_relevent_evidence_id += obtain_relevent_evidence_id (similarity_train)\n",
        "del similarity_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mzGjuYie7-RF"
      },
      "outputs": [],
      "source": [
        "dev_claims_relevent_evidence_id = obtain_relevent_evidence_id (similarity_dev)\n",
        "test_claims_relevent_evidence_id = obtain_relevent_evidence_id (similarity_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "z88MWtuKhCuO"
      },
      "outputs": [],
      "source": [
        "# obtain claim without tokenized and other preprosessing\n",
        "def obtain_original_claim (data):\n",
        "    claims_text = []\n",
        "    for key, value in data.items():\n",
        "        claims_text.append(value[\"claim_text\"])\n",
        "    return claims_text\n",
        "original_train_claims = obtain_original_claim(train_claims)\n",
        "original_dev_claims = obtain_original_claim(dev_claims)\n",
        "original_test_claims = obtain_original_claim(test_claims)\n",
        "original_evidence = []\n",
        "for key,value in evidences.items():\n",
        "    original_evidence.append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Izs2OW_hCuO",
        "outputId": "36c7d500-f964-4c6a-fa0d-9c7f67533e73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(texts):\n",
        "    for text in texts:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# create vocab dict, only word which appeared >= 5 will counted otherwise treat as unknown\n",
        "vocab = build_vocab_from_iterator(yield_tokens(evidence_text), specials=('<unk>', '<pad>', '<cls>'), min_freq = 10)\n",
        "\n",
        "# create vocab_cls using unprocessed evidence texts\n",
        "vocab_cls = build_vocab_from_iterator(yield_tokens(original_evidence), specials=('<unk>', '<pad>', '<cls>', '<sep>'), min_freq = 10)\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "vocab_cls.set_default_index(vocab[\"<unk>\"])\n",
        "padding_index = vocab[\"<pad>\"]\n",
        "cls_index = vocab[\"<cls>\"]\n",
        "sep_index = vocab[\"<sep>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXIxDGY1hCuO",
        "outputId": "06c110cc-c39c-46f2-99be-03a5c772dc9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53039\n",
            "66446\n"
          ]
        }
      ],
      "source": [
        "# vocab dict size\n",
        "print(len(vocab))\n",
        "print(len(vocab_cls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xnJpy1v4hCuO"
      },
      "outputs": [],
      "source": [
        "def convert_word_to_idx (data, vocab_dict):\n",
        "    res = []\n",
        "    for sentence in data:\n",
        "        indices = [vocab_dict[token] for token in tokenizer(sentence)]\n",
        "        res.append(indices)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9SAokC3GhCuO"
      },
      "outputs": [],
      "source": [
        "# convert token into idx, using processed data in retrieval later\n",
        "train_claims_text_idx = convert_word_to_idx(train_claims_text, vocab)\n",
        "dev_claims_text_idx = convert_word_to_idx(dev_claims_text, vocab)\n",
        "test_claims_text_idx = convert_word_to_idx(test_claims_text, vocab)\n",
        "evidence_text_idx = convert_word_to_idx(evidence_text, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NPC5tl9D9gXI"
      },
      "outputs": [],
      "source": [
        "# convert token into idx, using unprocessed data in classification later\n",
        "train_claims_origianl_text_idx = convert_word_to_idx(original_train_claims, vocab_cls)\n",
        "dev_claims_origianl_text_idx = convert_word_to_idx(original_dev_claims, vocab_cls)\n",
        "test_claims_origianl_text_idx = convert_word_to_idx(original_test_claims, vocab_cls)\n",
        "evidence_origianl_text_idx = convert_word_to_idx(original_evidence, vocab_cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqC-WokbhCuO",
        "outputId": "0f6289b7-dd95-4cdf-e250-0c6fca72397f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean length of train claims: 11.862377850162867\n",
            "mean length of dev claims: 12.474025974025974\n",
            "mean length of test claims: 11.660130718954248\n",
            "mean length of evidence: 11.273384032620053\n",
            "\n",
            "mean length of original train claims: 22.43241042345277\n",
            "mean length of original dev claims: 23.31168831168831\n",
            "mean length of original test claims: 22.398692810457515\n",
            "mean length of original evidence: 22.739117342680135\n"
          ]
        }
      ],
      "source": [
        "def cal_average_len(data):\n",
        "    total_length = sum(len(sentence) for sentence in data)\n",
        "    return total_length / len(data)\n",
        "\n",
        "print(\"mean length of train claims:\", cal_average_len(train_claims_text_idx))\n",
        "print(\"mean length of dev claims:\", cal_average_len(dev_claims_text_idx))\n",
        "print(\"mean length of test claims:\", cal_average_len(test_claims_text_idx))\n",
        "print(\"mean length of evidence:\", cal_average_len(evidence_text_idx))\n",
        "print()\n",
        "print(\"mean length of original train claims:\", cal_average_len(train_claims_origianl_text_idx))\n",
        "print(\"mean length of original dev claims:\", cal_average_len(dev_claims_origianl_text_idx))\n",
        "print(\"mean length of original test claims:\", cal_average_len(test_claims_origianl_text_idx))\n",
        "print(\"mean length of original evidence:\", cal_average_len(evidence_origianl_text_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MwMN4rwfhCuO"
      },
      "outputs": [],
      "source": [
        "#construct input for retrieval\n",
        "padded_claims_length = 25\n",
        "padded_evidence_length = 25\n",
        "def construct_input(data, padded_sentence_length):\n",
        "    res = []\n",
        "    for sentence in data:\n",
        "        if len(sentence) < padded_sentence_length:\n",
        "            res.append([vocab[\"<cls>\"]] + sentence + [vocab[\"<pad>\"]] * (padded_sentence_length - len(sentence)))\n",
        "        else:\n",
        "            res.append([vocab[\"<cls>\"]] + sentence[:padded_sentence_length])\n",
        "    return res\n",
        "\n",
        "train_input = construct_input(train_claims_text_idx, padded_claims_length)\n",
        "dev_input = construct_input(dev_claims_text_idx, padded_claims_length)\n",
        "test_input = construct_input(test_claims_text_idx, padded_claims_length)\n",
        "evidence_input = construct_input(evidence_text_idx, padded_evidence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KfX8lcoahCuO"
      },
      "outputs": [],
      "source": [
        "#construct input for classification\n",
        "padded_claims_length_cls = 50\n",
        "padded_evidence_length_cls = 50\n",
        "\n",
        "train_input_cls = construct_input(train_claims_origianl_text_idx, padded_claims_length_cls)\n",
        "dev_input_cls = construct_input(dev_claims_origianl_text_idx, padded_claims_length_cls)\n",
        "test_input_cls = construct_input(test_claims_origianl_text_idx, padded_claims_length_cls)\n",
        "evidence_input_cls = construct_input(evidence_origianl_text_idx, padded_evidence_length_cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeIuAaBf_bl4",
        "outputId": "fa59759c-efda-4b06-bf1d-02c78569f65e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "del evidence_text, dev_claims_text, train_claims_text, test_claims_text, original_train_claims, original_dev_claims, original_test_claims, original_evidence, train_claims_text_idx, dev_claims_text_idx, test_claims_text_idx, evidence_text_idx\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw6g5xAihCuO",
        "outputId": "bfdd02c4-ce16-4c6b-c66b-b97ca310b0a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# dataset for evidence retrieval\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, _input, correct_evidence_id, relevent_evidences_id, incorrect_evidenct_num):\n",
        "        self._input = _input\n",
        "        self.correct_evidence_id = correct_evidence_id\n",
        "        self.relevent_evidences_id = relevent_evidences_id\n",
        "        self.incorrect_evidenct_num = incorrect_evidenct_num\n",
        "        self._len = len(self._input)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self._input[idx], self.relevent_evidences_id[idx][100 - self.incorrect_evidenct_num:], self.correct_evidence_id[idx]]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    claims = []\n",
        "    evidences_id = []\n",
        "    correct_evidences_id = []\n",
        "\n",
        "    for claim, relevent_evidence, correct_evidence in batch:\n",
        "        claims.append(claim)\n",
        "        correct_evidences_id.append(correct_evidence)\n",
        "        evidences_id.extend(relevent_evidence + correct_evidence)\n",
        "\n",
        "    #remove duplicate\n",
        "    evidences_id = list(set(evidences_id))\n",
        "\n",
        "\n",
        "    evidences_to_inner_id = {j: idx for idx, j in enumerate(evidences_id)}\n",
        "    correct_evidences_inner_id = [[evidences_to_inner_id[j] for j in idx] for idx in correct_evidences_id]\n",
        "\n",
        "    evidences = [evidence_input[idx] for idx in evidences_id]\n",
        "\n",
        "    batch_encoded = {}\n",
        "    batch_encoded[\"claims\"] = torch.LongTensor(claims)\n",
        "    batch_encoded[\"evidences\"] = torch.LongTensor(evidences)\n",
        "    batch_encoded[\"correct_evidences_inner_id\"] = correct_evidences_inner_id\n",
        "\n",
        "    return batch_encoded\n",
        "\n",
        "train_dataset = CustomDataset(train_input, train_claims_evidence_id, train_claims_relevent_evidence_id, 10)\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=5, collate_fn = collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5b_jpBYghCuO"
      },
      "outputs": [],
      "source": [
        "def mapping_index(indices):\n",
        "    label_mapping = {\n",
        "        0: 'SUPPORTS',\n",
        "        1: 'REFUTES',\n",
        "        2: 'NOT_ENOUGH_INFO',\n",
        "        3: 'DISPUTED'\n",
        "    }\n",
        "    return [label_mapping[int(index)] for index in indices]\n",
        "\n",
        "\n",
        "def mapping_string(string):\n",
        "    if string == 'SUPPORTS':\n",
        "        return 0\n",
        "    if string == 'REFUTES':\n",
        "        return 1\n",
        "    if string == 'NOT_ENOUGH_INFO':\n",
        "        return 2\n",
        "    if string == 'DISPUTED':\n",
        "        return 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AUoCl7TFhCuO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "# dataset for classification\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, claims_text, positive_evidences_id, cls_label):\n",
        "        self.claims_text = claims_text\n",
        "        self.positive_evidences_id = positive_evidences_id\n",
        "        self.cls_label = cls_label\n",
        "        self.text_length = len(claims_text[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims_text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.claims_text[idx][:self.text_length], self.positive_evidences_id[idx], self.cls_label[idx]]\n",
        "\n",
        "# For one claim, combine the claim texts and 5 evidence texts as a list, repeat for all claims in batch and return a [[]].\n",
        "def collate_batch_CLS(batch):\n",
        "    queries = []\n",
        "    labels = []\n",
        "\n",
        "    for claim_texts, positive_evidences_id, label in batch:\n",
        "        # add cls to the front of texts denote start\n",
        "        temp_text = [vocab_cls[\"<cls>\"]]\n",
        "        temp_text.extend(claim_texts)\n",
        "\n",
        "        if len(positive_evidences_id) > 5:\n",
        "            for i in range(5):\n",
        "                temp_text.extend(evidence_input_cls[positive_evidences_id[i]])\n",
        "        else:\n",
        "            for i in positive_evidences_id:\n",
        "                temp_text.extend(evidence_input_cls[i])\n",
        "            # add padding to make up 5 * evidence length\n",
        "            padding_needed = (5 - len(positive_evidences_id)) * padded_evidence_length_cls\n",
        "            temp_text.extend([vocab_cls[\"<pad>\"]] * padding_needed)\n",
        "\n",
        "        # add sep to the end of texts denote finish\n",
        "        temp_text.extend([vocab_cls[\"<sep>\"]])\n",
        "\n",
        "        queries.append(temp_text)\n",
        "        labels.append(mapping_string(label))\n",
        "\n",
        "\n",
        "    max_length = max(len(q) for q in queries)\n",
        "    queries_padded = [q + [vocab_cls[\"<pad>\"]] * (max_length - len(q)) for q in queries]\n",
        "\n",
        "    batch_encoding = {}\n",
        "    batch_encoding[\"queries\"] = torch.LongTensor(queries_padded)\n",
        "    batch_encoding[\"labels\"] = torch.LongTensor(labels)\n",
        "\n",
        "    return batch_encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdGCi0pWhCuP"
      },
      "source": [
        "2.1. Retrieval Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "# tut8\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2Gk7cefLhCuP"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocabsize, modeldim, nhead, nhid, nlayers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(modeldim)\n",
        "        encoder_layers = TransformerEncoderLayer(modeldim, nhead, nhid, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=nn.LayerNorm(nhid))\n",
        "        self.encoder = nn.Embedding(vocabsize, modeldim)\n",
        "        self.modeldim = modeldim\n",
        "\n",
        "    def forward(self, claims):\n",
        "        _mask = claims == 1\n",
        "        claims_position = self.encoder(claims)\n",
        "        claims_position = self.pos_encoder(claims_position)\n",
        "        encoded = self.transformer_encoder(claims_position, src_key_padding_mask = _mask)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzeixXHLhCuP",
        "outputId": "e7b74243-1548-4d2b-fc96-6eb939b9f698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (encoder): Embedding(53039, 1024)\n",
              ")"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trans_encoder = TransformerModel(vocabsize=len(vocab), modeldim=1024, nhead=8, nhid=1024, nlayers=6)\n",
        "torch.cuda.empty_cache()\n",
        "trans_encoder.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_zjbKd_hCuP"
      },
      "source": [
        "2.2. Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GR3RaZ3JhCuP"
      },
      "outputs": [],
      "source": [
        "class CLS(nn.Module):\n",
        "    def __init__(self, vocabsize, modeldim, nhead, nhid, nlayers, output_size):\n",
        "        super(CLS, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(modeldim)\n",
        "        encoder_layers = TransformerEncoderLayer(modeldim, nhead, nhid, batch_first=True)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers, norm=nn.LayerNorm(nhid))\n",
        "        self.encoder = nn.Embedding(vocabsize, modeldim)\n",
        "        self.modeldim = modeldim\n",
        "        self.hidden_layer = nn.Linear(nhid, nhid // 2)\n",
        "        self.cls = nn.Linear(nhid // 2, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, claims):\n",
        "        _mask = claims == 1\n",
        "        claims_position = self.encoder(claims)\n",
        "        claims_position = self.pos_encoder(claims_position)\n",
        "        encoded = self.transformer_encoder(claims_position, src_key_padding_mask = _mask)\n",
        "        claims_cls = encoded[:, 0, :]\n",
        "        claims_hidden = F.tanh(self.hidden_layer(claims_cls))\n",
        "        self.dropout(claims_hidden)\n",
        "        result = self.cls(claims_hidden)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "6bUexZ3uhCuP"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_change=0.01):\n",
        "        self.patience = patience\n",
        "        self.min_change = min_change\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_change:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zlS3ZvsahCuP"
      },
      "outputs": [],
      "source": [
        "# Evidence retrival\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "retrieval_num = 5\n",
        "candidate_num = 10\n",
        "\n",
        "\n",
        "def validate(claims_input, evidence_idx, to_local_index, relevent_evidence_id, correct_evidence_id, model):\n",
        "    # get evidence embeddings\n",
        "    model.eval()\n",
        "    batch_size = 1000\n",
        "    evidence_len = len(evidence_idx[0])\n",
        "    text_len = len(claims_input[0])\n",
        "\n",
        "    # obtain the query embedding\n",
        "    # query = torch.LongTensor(claims_input).view(-1, text_len).cuda()\n",
        "    query = torch.LongTensor(claims_input).view(-1, text_len).to(device)\n",
        "    query_embedding = model(query)\n",
        "    query_embedding = query_embedding[:, 0, :].detach()\n",
        "    query_embedding = F.normalize(query_embedding, p=2, dim=1).cpu()\n",
        "\n",
        "    evidence_embeddings = []\n",
        "\n",
        "    # batch by batch to get all evidence embedding, if do in once will overflow\n",
        "\n",
        "    dataset = TensorDataset(torch.LongTensor(evidence_idx))\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "    evidence_embeddings = []\n",
        "    for batch in data_loader:\n",
        "        # cur_evidence = batch[0].cuda()\n",
        "        cur_evidence = batch[0].to(device)\n",
        "        cur_embedding = model(cur_evidence)\n",
        "        # need to put in cpu to prevent tensors in different devices, all in gpu might overflow\n",
        "        cur_embedding = cur_embedding[:, 0, :].detach().cpu()\n",
        "        cur_embedding = F.normalize(cur_embedding, p=2, dim=1)\n",
        "        evidence_embeddings.append(cur_embedding)\n",
        "\n",
        "    del dataset, data_loader\n",
        "\n",
        "    # concat the list of tensors into tensors\n",
        "    evidence_embeddings = torch.cat(evidence_embeddings, dim=0)\n",
        "\n",
        "    scores = torch.mm(query_embedding, evidence_embeddings.t())\n",
        "    f = []\n",
        "\n",
        "    # from eval.py\n",
        "    for i in range(scores.size(0)):\n",
        "        local_indices = [to_local_index[idx] for idx in relevent_evidence_id[i][:candidate_num]]\n",
        "        candidates_score = torch.index_select(scores[i], 0, torch.LongTensor(local_indices))\n",
        "        topk = torch.argsort(candidates_score, descending = True).tolist()\n",
        "        selected = topk[:retrieval_num]\n",
        "\n",
        "        evidence_correct = 0\n",
        "        pred_evidences = [relevent_evidence_id[i][j] for j in selected]\n",
        "        for evidence_id in correct_evidence_id[i]:\n",
        "            if evidence_id in pred_evidences:\n",
        "                evidence_correct += 1\n",
        "        if evidence_correct > 0:\n",
        "            evidence_recall = float(evidence_correct) / len(correct_evidence_id[i])\n",
        "            evidence_precision = float(evidence_correct) / len(pred_evidences)\n",
        "            evidence_fscore = (2 * evidence_precision * evidence_recall) / (evidence_precision + evidence_recall)\n",
        "        else:\n",
        "            evidence_fscore = 0\n",
        "        f.append(evidence_fscore)\n",
        "\n",
        "    fscore = np.mean(f)\n",
        "    print(\"Evidence Retrieval F-score: %.3f\" % fscore)\n",
        "    model.train()\n",
        "    return fscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "wamhzNCQ6UhU"
      },
      "outputs": [],
      "source": [
        "def filter_useful_evidences(data, relative_evidence_id):\n",
        "  to_local_index = {}\n",
        "  count = 0\n",
        "  res = []\n",
        "  for i in range(len(relative_evidence_id)):\n",
        "    # select the top candidate_num\n",
        "    for j in range(candidate_num):\n",
        "      if relative_evidence_id[i][j] not in to_local_index:\n",
        "        res.append(data[relative_evidence_id[i][j]])\n",
        "        to_local_index[relative_evidence_id[i][j]] = count\n",
        "        count += 1\n",
        "  return res, to_local_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzt7oae4hCuP",
        "outputId": "b1670384-dfef-4d4b-d2b3-a0c62fdde807",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/246 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 11%|█         | 26/246 [00:05<00:29,  7.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 5, avg loss: 4.263636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 51/246 [00:09<00:27,  7.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 10, avg loss: 4.351270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 76/246 [00:12<00:23,  7.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 15, avg loss: 4.249545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 99/246 [00:15<00:20,  7.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 20, avg loss: 4.203184\n",
            "\n",
            "Evaluate:\n",
            "\n",
            "Evidence Retrieval F-score: 0.069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 41%|████      | 100/246 [00:44<20:55,  8.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best f-score: 0.06900639043496189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 126/246 [00:47<00:16,  7.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 25, avg loss: 4.178691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████▏   | 151/246 [00:51<00:13,  7.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 30, avg loss: 4.164424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 176/246 [00:54<00:10,  6.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 35, avg loss: 4.184547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 199/246 [00:58<00:08,  5.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 40, avg loss: 4.116689\n",
            "\n",
            "Evaluate:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 201/246 [00:59<00:15,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score: 0.068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 226/246 [01:03<00:03,  5.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, steps: 45, avg loss: 4.162113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 246/246 [01:07<00:00,  3.65it/s]\n",
            "  2%|▏         | 5/246 [00:01<00:54,  4.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 1, avg loss: 4.022460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 30/246 [00:05<00:31,  6.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 6, avg loss: 4.106402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 53/246 [00:08<00:26,  7.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 11, avg loss: 4.108038\n",
            "\n",
            "Evaluate:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 55/246 [00:09<00:58,  3.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score: 0.065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 80/246 [00:13<00:23,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 16, avg loss: 4.042874\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 105/246 [00:16<00:20,  6.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 21, avg loss: 4.132701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 130/246 [00:20<00:17,  6.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 26, avg loss: 4.078761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 153/246 [00:23<00:13,  6.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 31, avg loss: 4.076507\n",
            "\n",
            "Evaluate:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 155/246 [00:24<00:28,  3.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score: 0.066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 180/246 [00:28<00:09,  7.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 36, avg loss: 4.117926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 205/246 [00:32<00:05,  6.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 41, avg loss: 4.090560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 230/246 [00:35<00:02,  6.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, steps: 46, avg loss: 4.064545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 246/246 [00:38<00:00,  6.39it/s]\n"
          ]
        }
      ],
      "source": [
        "#  Evidence retrival training\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "\n",
        "lr = 0.0005\n",
        "optimizer = torch.optim.SGD(trans_encoder.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "all_loss = []\n",
        "all_step = []\n",
        "step_accu = 0\n",
        "steps = 0\n",
        "\n",
        "torch.manual_seed(7)\n",
        "torch.cuda.manual_seed_all(7)\n",
        "random.seed(7)\n",
        "\n",
        "# parameter to contraol the temp status printout and model performance evaluation\n",
        "count = 0\n",
        "total_steps = 0\n",
        "step_length = 5\n",
        "report_freq = 5\n",
        "eval_interval = 20\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "save_path = os.path.join(current_dir, \"best_model.bin\")\n",
        "\n",
        "trans_encoder.train()\n",
        "avg_loss = 0\n",
        "optimizer.zero_grad()\n",
        "maximum_f_score = 0\n",
        "\n",
        "for epoch in range(2):\n",
        "    step_accu += steps\n",
        "    steps = 0\n",
        "\n",
        "    for (i, batch) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        count += 1\n",
        "    # forward pass\n",
        "        # claim_embeddings = trans_encoder(batch[\"claims\"].cuda())\n",
        "        # evidence_embeddings = trans_encoder(batch[\"evidences\"].cuda())\n",
        "        claim_embeddings = trans_encoder(batch[\"claims\"].to(device))\n",
        "        evidence_embeddings = trans_encoder(batch[\"evidences\"].to(device))\n",
        "\n",
        "        # reduce dimension from batch size * sentence len * embedding dimesion to batch size *  embedding by considering the first word\n",
        "        # since first word is the <cls> which have no other meaning, after training can accumuminate information in it, reduce the amount of calcualtion, also reduce the accuracy\n",
        "        claim_embeddings = claim_embeddings[:, 0, :]\n",
        "        evidence_embeddings = evidence_embeddings[:, 0, :]\n",
        "\n",
        "        # L2 normalization for every row\n",
        "        claim_embeddings = F.normalize(claim_embeddings, p=2, dim=1)\n",
        "        evidence_embeddings = F.normalize(evidence_embeddings, p=2, dim=1)\n",
        "\n",
        "        # calculate similarity by dot product\n",
        "        similarity = torch.mm(claim_embeddings, evidence_embeddings.t())\n",
        "\n",
        "        scores = - F.log_softmax(similarity / 0.1, dim=1)\n",
        "        scores_list = scores.tolist()\n",
        "    # backward pass\n",
        "        loss = []\n",
        "        for idx, correct_id in enumerate(batch[\"correct_evidences_inner_id\"]):\n",
        "            # correct_id = torch.LongTensor(correct_id).cuda()\n",
        "            correct_id = torch.LongTensor(correct_id).to(device)\n",
        "            # calculate the average score for each correct evidence in each claim\n",
        "            cur_loss = torch.mean(torch.index_select(scores[idx], 0, correct_id))\n",
        "            loss.append(cur_loss)\n",
        "        # stack the loss tensor then calculate mean of average loss, since they might have diffenent dimension\n",
        "        loss = torch.stack(loss).mean()\n",
        "        loss = loss / step_length\n",
        "        loss.backward()\n",
        "        avg_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if count == step_length:\n",
        "            count = 0\n",
        "            steps += 1\n",
        "            total_steps += 1\n",
        "\n",
        "        if total_steps % report_freq == 0 and count == 0:\n",
        "            print(\"epoch: %d, steps: %d, avg loss: %.6f\" % (epoch, steps, avg_loss / report_freq))\n",
        "            all_step.append(step_accu + steps)\n",
        "            all_loss.append(avg_loss/ report_freq)\n",
        "            avg_loss = 0\n",
        "        del loss, similarity, claim_embeddings, evidence_embeddings\n",
        "\n",
        "        # test current model with dev dataset\n",
        "        if total_steps % eval_interval == 0 and count == 0:\n",
        "            print(\"\\nEvaluate:\\n\")\n",
        "            select_evidences, to_local_index = filter_useful_evidences(evidence_input, dev_claims_relevent_evidence_id)\n",
        "            f_score = validate(dev_input, select_evidences, to_local_index, dev_claims_relevent_evidence_id, dev_claims_evidence_id, trans_encoder)\n",
        "            if f_score > maximum_f_score:\n",
        "                maximum_f_score = f_score\n",
        "                torch.save(trans_encoder.state_dict(), save_path)\n",
        "                print(\"best f-score:\", f_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "iy7ErDTjhCuP",
        "outputId": "dc731c2d-f581-4c86-ac1f-299a91395475"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'average loss')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(all_step, all_loss)\n",
        "plt.title('Change in average loss during training')\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('average loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CCChpBmIhCuP"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "def train(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['queries'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['queries'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs,labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=-1)  # Directly call argmax on outputs\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "GUhG6lWAhCuP"
      },
      "outputs": [],
      "source": [
        "# Classification\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "def cross_validate_model(model, optimizer, criterion, dataset, num_splits=5, batch_size=32, num_epochs=4):\n",
        "    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "    best_model = None\n",
        "    best_accuracy = 0.0\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    stop = False\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
        "        if stop:\n",
        "            break\n",
        "\n",
        "        print(f\"Folding {fold+1}/{num_splits}\")\n",
        "        train_subset = Subset(dataset, train_idx)\n",
        "        val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_CLS)\n",
        "        valid_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch_CLS)\n",
        "        early_stopping = EarlyStopping(patience=3, min_change=0.001)\n",
        "        best_fold_model = None\n",
        "        best_fold_accuracy = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = train(model, train_loader, optimizer, criterion)\n",
        "            eval_loss, eval_accuracy = evaluate(model, valid_loader, criterion)\n",
        "            print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {eval_loss}, Accuracy: {eval_accuracy}')\n",
        "            dev_loader = DataLoader(dev_dataset_CLS, batch_size=10, shuffle=True, collate_fn=collate_batch_CLS)\n",
        "            dev_loss, dev_accuracy = evaluate(model, dev_loader, criterion)\n",
        "            print(f\"dev_accuray is {dev_accuracy}\")\n",
        "\n",
        "            if dev_accuracy > 0.60:\n",
        "                print(f\"Stopping early: dev_accuracy = {dev_accuracy} > 0.6\")\n",
        "                stop = True\n",
        "                break\n",
        "\n",
        "            if eval_accuracy > best_fold_accuracy:\n",
        "                best_fold_accuracy = eval_accuracy\n",
        "\n",
        "            early_stopping(eval_loss)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "        results.append((eval_loss, eval_accuracy))\n",
        "        print(f\"Validation Loss: {eval_loss:.4f}, Accuracy: {eval_accuracy:.4f}\")\n",
        "\n",
        "        if best_fold_accuracy > best_accuracy:\n",
        "            best_accuracy = best_fold_accuracy\n",
        "\n",
        "    avg_loss = sum([x[0] for x in results]) / num_splits\n",
        "    avg_accuracy = sum([x[1] for x in results]) / num_splits\n",
        "\n",
        "    return avg_loss, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8x-H1nQKhCuQ"
      },
      "outputs": [],
      "source": [
        "train_dataset_CLS = TrainDataset(train_input_cls,train_claims_evidence_id,train_claims_label)\n",
        "dev_dataset_CLS = TrainDataset(dev_input_cls, dev_claims_evidence_id, dev_claims_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nowLpii7hCuQ"
      },
      "outputs": [],
      "source": [
        "cls_model = CLS(vocabsize=len(vocab_cls), modeldim=512, nhead=8, nhid=512, nlayers=6, output_size=4)\n",
        "cls_model.to(device)\n",
        "optimizer = torch.optim.Adam(cls_model.parameters(), lr=0.00005)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITrcI1VIhCuQ",
        "outputId": "0ec283a2-f5f1-4772-c7bb-e28b59f3d213",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folding 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.291153738575597, Validation Loss: 1.2599556893110275, Accuracy: 0.45934959349593496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.44805194805194803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 1.256526497102553, Validation Loss: 1.2522450238466263, Accuracy: 0.47560975609756095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.44805194805194803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.51it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 1.2520476195120043, Validation Loss: 1.2333583235740662, Accuracy: 0.45934959349593496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 38.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.44155844155844154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.47it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 1.2298034410322867, Validation Loss: 1.1807177513837814, Accuracy: 0.5203252032520326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 38.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.538961038961039\n",
            "Validation Loss: 1.1807, Accuracy: 0.5203\n",
            "Folding 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.47it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.1469873170698843, Validation Loss: 1.1065945625305176, Accuracy: 0.5650406504065041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.45454545454545453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.49it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 1.080128498615757, Validation Loss: 1.018149845302105, Accuracy: 0.5609756097560976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 38.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.5714285714285714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.51it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 1.0521365461810943, Validation Loss: 1.102496363222599, Accuracy: 0.5284552845528455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 38.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.5454545454545454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.52it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 13.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 1.0595458688274506, Validation Loss: 1.1710958182811737, Accuracy: 0.45121951219512196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.5064935064935064\n",
            "Validation Loss: 1.1711, Accuracy: 0.4512\n",
            "Folding 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.03174041932629, Validation Loss: 1.0470609590411186, Accuracy: 0.5609756097560976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.5714285714285714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.53it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 1.014559515060917, Validation Loss: 0.982265017926693, Accuracy: 0.5934959349593496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 38.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.5454545454545454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.52it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 13.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 0.9747637414163158, Validation Loss: 1.0404684096574783, Accuracy: 0.573170731707317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.551948051948052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 31/31 [00:08<00:00,  3.50it/s]\n",
            "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 12.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 0.9887466642164415, Validation Loss: 0.9276233240962029, Accuracy: 0.6056910569105691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 39.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev_accuray is 0.6103896103896104\n",
            "Stopping early: dev_accuracy = 0.6103896103896104 > 0.6\n",
            "Validation Loss: 0.9276, Accuracy: 0.6057\n",
            "Average Validation Loss: 0.6559, Average Accuracy: 0.3154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "avg_loss, avg_accuracy = cross_validate_model(cls_model, optimizer, criterion, train_dataset_CLS)\n",
        "print(f'Average Validation Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efCwSl7GhCuQ",
        "outputId": "6bd45434-601c-4034-c59a-ae9d6c3cd0e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 16/16 [00:00<00:00, 33.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Development Loss: 0.9980, Accuracy: 0.6104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dev_loader = DataLoader(dev_dataset_CLS, batch_size=10, shuffle=True, collate_fn=collate_batch_CLS)\n",
        "dev_loss, dev_accuracy = evaluate(cls_model, dev_loader, criterion)\n",
        "print(f'Development Loss: {dev_loss:.4f}, Accuracy: {dev_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMcCab4DhCuQ"
      },
      "source": [
        "Retrieval Modelc Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eywFqkBhCuQ",
        "outputId": "e567aad7-4104-40b0-932a-81aa9bdf185c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (encoder): Embedding(53039, 1024)\n",
              ")"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the best performed model in dev dataset\n",
        "torch.cuda.empty_cache()\n",
        "trans_encoder.load_state_dict(torch.load(save_path))\n",
        "trans_encoder.to(device)\n",
        "trans_encoder.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ylcr0YARhCuQ"
      },
      "outputs": [],
      "source": [
        "retrieval_num = 5\n",
        "candidate_num = 6\n",
        "\n",
        "def evidence_predicts(claims_input, relevent_evidence_id, evidences_id, model):\n",
        "    # obtain evidence embeddings\n",
        "    batch_size = 1000\n",
        "    select_evidences, to_local_index = filter_useful_evidences(evidence_input, relevent_evidence_id)\n",
        "    dataset = TensorDataset(torch.LongTensor(select_evidences))\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "    evidence_embeddings = []\n",
        "    for batch in data_loader:\n",
        "        # cur_evidence = batch[0].cuda()\n",
        "        cur_evidence = batch[0].to(device)\n",
        "        cur_embedding = trans_encoder(cur_evidence)\n",
        "        # need to put in cpu to prevent tensors in different devices, all in gpu might overflow\n",
        "        cur_embedding = cur_embedding[:, 0, :].detach().cpu()\n",
        "        cur_embedding = F.normalize(cur_embedding, p=2, dim=1)\n",
        "        evidence_embeddings.append(cur_embedding)\n",
        "\n",
        "    del dataset, data_loader\n",
        "\n",
        "    # concat the list of tensors into tensors\n",
        "    evidence_embeddings = torch.cat(evidence_embeddings, dim=0).t()\n",
        "\n",
        "    text_len = len(claims_input[0])\n",
        "    model.eval()\n",
        "\n",
        "    # query = torch.LongTensor(claims_input).view(-1, text_len).cuda()\n",
        "    query = torch.LongTensor(claims_input).view(-1, text_len).to(device)\n",
        "    query_embedding = model(query)\n",
        "    query_embedding = query_embedding[:, 0, :].detach()\n",
        "    query_embedding = F.normalize(query_embedding, p=2, dim=1).cpu()\n",
        "\n",
        "    scores = torch.mm(query_embedding, evidence_embeddings)\n",
        "    pred = []\n",
        "    for i in range(scores.size(0)):\n",
        "        local_indices = [to_local_index[idx] for idx in relevent_evidence_id[i][:candidate_num]]\n",
        "        candidates_score = torch.index_select(scores[i], 0, torch.LongTensor(local_indices))\n",
        "        topk = torch.argsort(candidates_score, descending = True).tolist()\n",
        "        selected = topk[:retrieval_num]\n",
        "\n",
        "        pred_evidences = [evidences_id[relevent_evidence_id[i][j]] for j in selected]\n",
        "        pred.append(pred_evidences)\n",
        "\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "lJyQdeAXhCuQ"
      },
      "outputs": [],
      "source": [
        "pred_test_evidence = evidence_predicts(test_input, test_claims_relevent_evidence_id, evidence_id, trans_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Qm7dJRswhCuQ"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for key, value in test_claims.items():\n",
        "    value[\"evidences\"] = pred_test_evidence[i]\n",
        "    i += 1\n",
        "json.dump(test_claims, open(\"test_claims.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3kZAY2KhCuQ"
      },
      "source": [
        "Classification Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "dir_path = 'test_claims.json'\n",
        "\n",
        "\n",
        "with open(dir_path, 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "XgIX5v8yhCuQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "claims = []\n",
        "evidences = []\n",
        "\n",
        "\n",
        "for claim_id, claim_data in data.items():\n",
        "    claim_text = claim_data['claim_text']\n",
        "    evidence_ids = claim_data['evidences']\n",
        "\n",
        "\n",
        "    claims.append(claim_text)\n",
        "    evidences.append(evidence_ids)\n",
        "\n",
        "evidences = obtain_evidence_id(evidences)\n",
        "\n",
        "def collate_batch_cls(batch):\n",
        "    queries = []\n",
        "\n",
        "\n",
        "    for claim_texts, positive_evidences_id in batch:\n",
        "        # Add cls to the front of texts to denote start\n",
        "        temp_text = [vocab_cls[\"<cls>\"]]  # Assuming <cls> token is 2\n",
        "        temp_text.extend(claim_texts)\n",
        "\n",
        "        if len(positive_evidences_id) > 5:\n",
        "            for i in range(5):\n",
        "                temp_text.extend(evidence_input_cls[positive_evidences_id[i]])\n",
        "        else:\n",
        "            for i in positive_evidences_id:\n",
        "                temp_text.extend(evidence_input_cls[i])\n",
        "            # Add padding to make up 5 * evidence length\n",
        "            padding_needed = (5 - len(positive_evidences_id)) * padded_evidence_length_cls\n",
        "            temp_text.extend([vocab_cls[\"<pad>\"]] * padding_needed)\n",
        "\n",
        "\n",
        "        temp_text.extend([vocab_cls[\"<sep>\"]])  # Assuming <sep> token is 3\n",
        "\n",
        "        queries.append(temp_text)\n",
        "\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_length = max(len(q) for q in queries)\n",
        "    queries_padded = [q + [vocab_cls[\"<pad>\"]] * (max_length - len(q)) for q in queries]\n",
        "\n",
        "    batch_encoding = {}\n",
        "    batch_encoding[\"queries\"] = torch.LongTensor(queries_padded)\n",
        "\n",
        "    return batch_encoding\n",
        "\n",
        "\n",
        "class testDataset(Dataset):\n",
        "    def __init__(self, claims_text, positive_evidences_id):\n",
        "        self.claims_text = claims_text\n",
        "        self.positive_evidences_id = positive_evidences_id\n",
        "        self.text_length = len(claims_text[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claims_text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.claims_text[idx][:self.text_length], self.positive_evidences_id[idx]]\n",
        "\n",
        "test_dataset_CLS = testDataset(test_input_cls,evidences)\n",
        "test_loader = DataLoader(test_dataset_CLS, batch_size=5, shuffle=True, collate_fn=collate_batch_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaq-QSWxhCuQ",
        "outputId": "4cd38905-b34c-4f71-9029-33893c1d62f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 31/31 [00:00<00:00, 61.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({0: 93, 2: 60})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "def label_predict(loader, model):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Predicting\"):\n",
        "            queries = batch[\"queries\"].to(device)\n",
        "            outputs = model(queries)\n",
        "            preds = torch.argmax(outputs, dim=1).tolist()\n",
        "            predictions.extend(preds)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = label_predict(test_loader, cls_model)\n",
        "print(Counter(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dCw59K8khCuR"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "predictions = mapping_index(predictions)\n",
        "for key, value in test_claims.items():\n",
        "    value[\"claim_label\"] = predictions[i]\n",
        "    i+=1\n",
        "json.dump(test_claims, open(\"test_claims.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
